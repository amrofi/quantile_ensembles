---
title: "Quantile forecasting with ensembles"
author: Rob J Hyndman
branding: false
bibliography: refs.bib
output: MonashEBSTemplates::memo
numbersections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, warning=FALSE, message=FALSE)

library(fpp3)
library(distributional)
set.seed(20200723)

if(file.exists("cafe.rds")) {
  cafe <- readRDS("cafe.rds")
} else {
  cafe <- readabs::read_abs(series_id = "A3349870V") %>%
    select(date, value) %>%
    mutate(date = yearmonth(date)) %>%
    as_tsibble(index=date) %>%
    filter(date >= yearmonth("2006 Jan"),
           date <= yearmonth("2019 Dec"))
  saveRDS(cafe,"cafe.rds")
}
```

# Quantile forecasting

Almost everyone needs probabilistic forecasts whether they realise it or not. Without some kind of probabilistic forecast or other measure of uncertainty, a point forecast is largely useless as there is no way of knowing how wrong it is likely to be. A simple version of a probabilistic forecast is a prediction interval which is intended to cover the true value with a specified probability. Another type of probabilistic forecast is the notion of "safety stock", which is the additional stock to be ordered above the point forecast in order to meet demand with a specified probability. 

A more sophisticated way of producing probabilistic forecasts is to generate quantile forecasts. For example, a 90% quantile forecast is a value which should exceed the true observation 90% of the time, and be less than the true value 10% of the time. Median forecasts are equivalent to 50% quantile forecasts. Prediction intervals are often constructed in this way --- an 80% prediction interval can be based on the 10% and 90% quantile forecasts.  Safety stock can also be computed from quantile forecasts --- set the stock order to be a 95% quantile to ensure your probability of being out-of-stock is 5%.

Quantile forecasting is not difficult. Any statistical forecasting method can be used to produce quantile forecasts by simulation. Suppose we are interested in forecasting the total sales in Australian cafes and we train an ETS model and an ARIMA model [@fpp3] on the data to the end of 2018. Then we can simulate sample paths from these models to obtain many possible "futures". Figure \@ref(fig:samples) shows the last four years of training data and 3 futures generated from each of the two fitted models. 

```{r samples, echo=FALSE, fig.cap="Future sample paths obtained using an ARIMA model and an ETS model for the Australian monthly cafe turnover.", fig.height=4, fig.width=7}
train <- cafe %>% filter(year(date) <= 2018)
fit <- train %>% 
  model(
    ETS = ETS(value),
    ARIMA = ARIMA(value)
  )
future <- fit %>%
  generate(times=3, h="1 year")
train %>% filter(year(date) >= 2015) %>% autoplot(value) +
  #geom_line(data = cafe %>% filter(year(date) == 2019))  +
  geom_line(data=future %>% mutate(modrep = paste0(.model,.rep)), aes(y=.sim, col=.model, group=c(modrep))) +
  labs(x="Month", y="Turnover (A$million)") +
  guides(colour = guide_legend("Model"))
```

If we repeat this procedure thousands of times for each model, we can obtain a very clear picture of the probability distribution for each future time period. The means of these distributions are the traditional point forecasts. Traditional 95% prediction intervals are equivalent to finding the middle 95% of the futures at each forecast horizon. Using simulations gives us the ability to easily compute any other quantiles. Figure \@ref(fig:quantiles) shows the deciles for the ETS forecasts (i.e., the 10th, 20th, \dots, 90th percentiles).

```{r quantiles, dependson='samples', fig.cap="Blue: Deciles for the ETS forecasts for the Australian monthly cafe turnover. Black: Observed values.", fig.height=4, fig.width=6}
qf <- fit %>%
  select(ETS) %>%
  generate(times=1000, h="1 year") %>%
  as_tibble() %>%
  group_by(date) %>%
  summarise(
    qs = quantile(.sim, seq(from=0.1, to=0.9, by=0.1)), prob=seq(from=0.1, to=0.9, by=0.1)
  ) 
qf %>%
  ggplot(aes(x=date)) +
  geom_line(aes(y=qs, group=prob), col='blue', alpha=0.5) +
  geom_line(aes(y=value), data=cafe %>% filter(year(date) == 2019)) +
  geom_line(aes(y=value), data=train %>% filter(year(date) >= 2015)) +
  labs(x="Month", y="Turnover (A$million)")
```

Quantile forecasts also allow us to answer many more interesting questions. For example, we may wish to find prediction intervals for the total turnover for the next 12 months. This is surprisingly difficult to handle analytically but trivial using simulations --- we just need to add up the turnover for each of the simulated sample paths, and then find the relevant quantiles. We might also want to  forecast the maximum turnover in any month over the next year. Again, that is a difficult problem analytically, but very easy using simulations. I expect that simulating future sample paths will play an increasingly important role in forecasting practice because it makes difficult problems relatively easy, and allows us to explore what the future might be like in ways that would otherwise be almost impossible.

Using simulations in forecasting requires a generative statistical model to be used. This is easy using an ARIMA or ETS model, but more difficult if something like a neural network or random forest has been used.

# Evaluating quantile forecasts

Most business doing forecasting will be familiar with computing accuracy measures for point forecasts such as MAPE or RMSE values. With quantile forecasts, we need to use some alternative measures. 

Quantile scores provides a measure of accuracy for each quantile of interest. For example, suppose we are interested in the quantile with probability $p$ for the future time periods, and let this be denoted by $q^{p}_{T+h|T}$, meaning the estimated quantile for time $T+h$ made at time $T$. That is, we expect the observation at time $T+h$ to be less than this value with probability $p$. For example, an estimate of the 95th percentile would be $q^{0.95}_{T+h|T}$. If $y_{T+h}$ denotes the observation at time $T+h$, then the quantile score is
$$
  Q_p = \begin{cases} 
  2(1 - p) \big(q^{p}_{T+h|T} - y_{T+h}\big), & \text{if $y_{T+h} < q^{p}_{T+h|T}$}\\ 
  2p \big(y_{T+h} - q^{p}_{T+h|T}\big), & \text{if $y_{T+h} \ge q^{p}_{T+h|T}$} \end{cases} 
$$
This is sometimes called the "pinball loss function" because a graph of it resembles a pinball table. The multiplier of 2 is often omitted, but including it makes the interpretation a little easier. A low value of $Q_p$ indicates a better estimate of the quantile.

```{r qp, dependson='quantiles'}
fcast <- qf %>% filter(prob==0.9, date==yearmonth("2019 Dec")) %>% pull(qs)
actual <- cafe %>% filter(date==yearmonth("2019 Dec")) %>% pull(value)
```

In Figure \@ref(fig:quantiles), the 90% quantile forecast for December 2019 is $q^{0.90}_{T+h|T} = `r round(fcast)`$ and the observed value is `r round(actual)`. Then 
$Q_{0.9} = 2(1-0.9) (`r round(fcast)` - `r round(actual)`) = `r round(2*(1-0.9) *(fcast - actual))`$. This can be interpreted like an absolute error. In fact, when $p=0.5$, the quantile score $Q_{0.5}$ is the same as the absolute error.

A percentage error variation of quantile scores can be obtained by dividing by the median point forecast $q^{0.5}_{T+h|T}$. This gives the quantile score as a percentage of the median. As with percentage errors used elsewhere, this only makes sense to use when percentages are meaningful. If the point forecasts are close to zero, or if the variable being forecast is not on a ratio scale, percentages should not be used [@fpp2].

If $p>0.5$, $Q_p$ gives a heavier penalty when the observation is greater than the estimated quantile than when the observation is less than the estimated quantile. The reverse is true for $p<0.5$. Usually we would want to average the $Q_p$ values of many forecast horizons in order to assess whether our model is estimating the quantiles accurately.

Often we are interested in the whole forecasting distribution, and then we can average the quantile scores over all values of $p$. This gives what is known as the "Continuous Ranked Probability Score" or CRPS. 

In the Australian cafe example, we can compute the CRPS values over the 12 months of 2019 for each of the ARIMA and ETS models. To make it more interpretable, we can also compute the CRPS for a simple seasonal naive model, and then we can calculate the "skill score" equal to the percentage improvement for ARIMA and ETS over seasonal naive.

```{r crps, dependson='samples'}
fcasts <- train %>% 
  model(
    ETS = ETS(value),
    ARIMA = ARIMA(value),
    SNAIVE = SNAIVE(value)
  ) %>%
  forecast(h = "1 year")
crps <- fcasts %>%
  accuracy(cafe, measures=list(CRPS=CRPS))
snaive_crps <- crps %>% filter(.model=="SNAIVE") %>% pull(CRPS)
crps <- crps %>%
  mutate(skillscore = 100*(1 - CRPS/snaive_crps))
crps %>%
  select(-.type) %>%
  rename(
    Model = .model,
    `Skill score` = skillscore
  ) %>%
  knitr::kable(booktabs=TRUE, digits=1)
```

Here, ETS is providing the best quantile forecasts with a skill score of `r sprintf("%.1f", crps %>% filter(.model=="ETS") %>% pull(skillscore))`. 

# Ensemble forecasting

For more than 50 years we have known that ensemble forecasting improves forecast accuracy [@Bates1969-dp;@Clemen1989-fz]. Also known as "combination forecasting", ensembles involve using multiple models and combining the results to produce the final forecast. There are several reasons why ensembles work.

  1. No model is perfect, and the data did not come from a model. As George Box has put it, “all models are wrong, but some are useful.” Ensembles allow the good features of various models to be included, while reducing the impact of any specific model.
  2. Ensembles decrease the variance of the forecasts [@Hibon2005-cv] by reducing the uncertainty associated with selecting a particular model.

Ensembling with probabilistic forecasting is easy when we use simulated future sample paths. We can simply combine the simulations from all models. If a weighted ensemble is needed, we can make the number of simulations from each model correspond to the required weight.

For the cafe example discussed here, we can combine 1000 simulated sample paths from each of the ETS and ARIMA models, and compute the resulting quantile forecasts. This automatically takes account of any correlations between the forecasts which can be tricky to handle analytically.

```{r ensemble, dependson='samples'}
future <- fit %>%
  generate(times=1000, h="1 year") %>%
  as_tibble() %>%
  select(-.rep, -.model) %>%
  nest(sample = c(-date)) %>%
  group_by(date) %>%
  mutate(
    sample = list(unname(unlist(sample))),
    value = dist_sample(sample),
    .mean = mean(value),
    .model = "ensemble"
  ) %>%
  ungroup() %>%
  select(-sample) %>%
  as_fable(index=date, key = '.model', distribution = value, response="value") 
  bind_rows(fcasts)
future %>%
  accuracy(cafe, measures=list(CRPS=CRPS)) %>%
  mutate(skillscore = 100*(1 - CRPS/snaive_crps)) %>%
  select(-.type) %>%
  rename(
    Model = .model,
    `Skill score` = skillscore
  ) %>%
  knitr::kable(booktabs=TRUE, digits=1)
```

